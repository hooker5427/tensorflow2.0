{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./test.csv' does not exist: b'./test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-75f80a59e46a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m            'Ticket' ,'Fare' ,'Cabin','Embarked' ]\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m   \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtest_data\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m   \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./test.csv' does not exist: b'./test.csv'"
     ]
    }
   ],
   "source": [
    "train_path  = './train.csv' \n",
    "test_path = './test.csv'\n",
    "\n",
    "columns = [ 'PassengerId','Survived' ,'Pclass' ,'Name','Sex','Age', 'SibSp','Parch' ,\n",
    "           'Ticket' ,'Fare' ,'Cabin','Embarked' ]\n",
    "train_data = pd.read_csv(train_path ,  header = None , skiprows =1   )  \n",
    "test_data  = pd.read_csv(test_path ,  header = None , skiprows =1   )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = columns \n",
    "test_data.columns = [ 'PassengerId'  ,'Pclass' ,'Name','Sex','Age', 'SibSp','Parch' ,'Ticket' ,'Fare' ,'Cabin','Embarked' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.concat(  [ train_data  ,test_data ]  , axis  =0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display ( data.head() , data.tail() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "index  = pd.isna( data['Survived'])\n",
    "data['is_test'] = index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data.isnull()) /len( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop( ['Cabin' ,'Name' ]  ,axis  =1 ,inplace= True ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].fillna( data['Age'].mean() , inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1(s) :\n",
    "    if (s <= 1.0) :\n",
    "        return 0\n",
    "    elif s<=5.0 :\n",
    "        return 1 \n",
    "    elif s <=18.0:\n",
    "        return 2 \n",
    "    elif s<=35.0 :\n",
    "        return 3 \n",
    "    elif s<=50.0:\n",
    "        return 4 \n",
    "    else :\n",
    "        return 5 \n",
    "    return 1 \n",
    "data['age_bin'] = data['Age'].apply(fun1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fare'].fillna(data['Fare'].mean(), inplace=True)\n",
    "data['Fare'].fillna(data['Fare'].mean(),inplace=True)\n",
    "# 使用登录最多的港口来填充登录港口的nan值\n",
    "data['Embarked'].fillna('S', inplace=True)\n",
    "data['Embarked'].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['family_size'] = data['SibSp'] + data['Parch'] +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['average_Fare'] =data['Fare'] /data['family_size']\n",
    "data['average_Fare'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['average_Fare_bin'] = pd.cut( data['average_Fare'] ,10 )\n",
    "data['average_Fare_bin'] = pd.cut( data['average_Fare'] ,10 )\n",
    "data['average_Fare_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['average_Fare_bin'] = pd.Categorical( data['average_Fare_bin']).codes\n",
    "data['average_Fare_bin'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['average_Fare_bin'] .unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun2(serices ):\n",
    "    for line in serices :\n",
    "        line = line.upper().strip() \n",
    "        if  line <='9' and line >='0' : \n",
    "            return 'U'\n",
    "        else :\n",
    "            return line[0]\n",
    "data['Ticket'] = data['Ticket'].apply( fun2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data.isnull()) /len( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.loc[ data['is_test']== False ,:] \n",
    "train_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.loc[ data['is_test'],:] \n",
    "test_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.countplot( x = \"age_bin\" , hue= 'Survived' , data = train_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot( x = \"Embarked\" , hue= 'Survived' , data = train_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot( x = \"family_size\" , hue= 'Survived' , data = train_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot( x = \"Pclass\" , hue= 'Survived' , data = train_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot( x = \"Sex\" , hue= 'Survived' , data = train_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encodes \n",
    "one_hot_cols  =  [   \"Pclass\" ,\"Sex\" ,\"SibSp\" ,\"Parch\"  ,\"average_Fare_bin\" ,'Embarked' ,'family_size' , 'age_bin'  ,'Ticket']\n",
    "data_bins = train_data.loc[:, one_hot_cols  ]\n",
    "dataset_bin_enc = pd.get_dummies( data_bins , columns=one_hot_cols  )\n",
    "dataset_bin_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Ticket' , 'Parch', 'Fare', 'Embarked','family_size', 'average_Fare']\n",
    "selected_data =  train_data.loc[: ,train_label  ]\n",
    "\n",
    "for tag  in ['Sex' ,'Embarked' ,'Ticket' , 'family_size'] :\n",
    "    selected_data[tag] = pd.Categorical(selected_data[tag] ).codes \n",
    "    \n",
    "Xtrain = selected_data.loc[:,train_label].drop('Survived' ,axis =1 ).loc[:750 , :] \n",
    "ytrain = selected_data.loc[:750 , 'Survived']\n",
    "\n",
    "Xvalid = selected_data.loc[:,train_label].drop('Survived' ,axis =1 ).loc[751: , :] \n",
    "yvalid = selected_data.loc[751:, 'Survived']\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(selected_data.loc[:  , train_label].drop('Survived', axis=1), selected_data['Survived'])\n",
    "\n",
    "print (\"RandomForestClassifier id runing\\n \")\n",
    "\n",
    "pred = clf.predict(Xtrain )\n",
    "accuracy = np.sum(pred == ytrain ) / pred.shape[0] # 打分 就是准确率\n",
    "print('Train accuracy = {}'.format(accuracy))\n",
    "\n",
    "pred = clf.predict(Xvalid)\n",
    "accuracy = np.sum(pred == yvalid) / pred.shape[0] # 打分 就是准确率\n",
    "print('Test accuracy = {}'.format(accuracy))\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "importance = clf.feature_importances_\n",
    "importance = pd.DataFrame(importance, index=selected_data.loc[:  , train_label].drop('Survived', axis=1).columns, columns=[\"Importance\"])\n",
    "importance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "test_feature = [ 'Pclass', 'Sex', 'Age', 'SibSp', 'Ticket' , 'Parch', 'Fare', 'Embarked','family_size', 'average_Fare']\n",
    "test_feature_data =  test_data.loc[: ,test_feature ]\n",
    "for tag  in ['Sex' ,'Embarked' ,'Ticket' , 'family_size'] :\n",
    "    test_feature_data[tag] = pd.Categorical(test_feature_data[tag] ).codes \n",
    "    \n",
    "last_test_id  = data.loc[ data['is_test'] ,'PassengerId' ]\n",
    "pred = clf.predict(test_feature_data )\n",
    "\n",
    "pred.astype(np.int32)\n",
    "\n",
    "res = dict()\n",
    "print (\" test data   save \\n\")\n",
    "for i in range(len( pred)) : \n",
    "    print (  last_test_id[i] ,  int(pred[i]))\n",
    "    res[last_test_id[i] ] =  pred [i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier #导入GBGT \n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "gbdt = GradientBoostingClassifier(\n",
    "                                n_estimators=10,   #建立十颗树 \n",
    "                                 learning_rate=0.1, # 学习率\n",
    "                                 max_depth=3 #每棵树最大的深度 \n",
    ")  \n",
    "\n",
    "print (\"gbgt is runing \")\n",
    "# start training\n",
    "start_time = time.time()\n",
    "gbdt.fit(Xtrain, ytrain)\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "pred = gbdt.predict(Xtrain)\n",
    "accuracy = np.sum(pred == ytrain) / pred.shape[0] # 打分 就是准确率\n",
    "print('Train accuracy = {}'.format(accuracy))\n",
    "# prediction and evaluation \n",
    "pred = gbdt.predict(Xvalid)\n",
    "accuracy = np.sum(pred == yvalid) / pred.shape[0] # 打分 就是准确率\n",
    "print('Test accuracy = {}'.format(accuracy))\n",
    "# 使用gbdt算法看特征值重要性\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "importance = gbdt.feature_importances_\n",
    "importance = pd.DataFrame(importance, index=data.loc[:  , train_label].drop('Survived', axis=1).columns, columns=[\"Importance\"])\n",
    "importance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)/2) ,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论就是 ['Pclass', 'Sex', 'Age', 'Ticket' ,  'Fare', 'Embarked','family_size', 'average_Fare'] 比较重要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from  sklearn.neighbors import KNeighborsClassifier\n",
    "from  sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_test, preds):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_ml_algo(algo, X_train, y_train, X_test, y_test ,  cv):\n",
    "    # One Pass\n",
    "    model = algo.fit(X_train, y_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    if (isinstance(algo, (LogisticRegression, \n",
    "                          KNeighborsClassifier, \n",
    "                          GaussianNB, \n",
    "                          DecisionTreeClassifier, \n",
    "                          RandomForestClassifier,\n",
    "                          GradientBoostingClassifier))):\n",
    "        probs = model.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        probs = \"Not Available\"\n",
    "    acc = round(model.score(X_test, y_test) * 100, 2) \n",
    "    # CV \n",
    "    train_pred = model_selection.cross_val_predict(algo, \n",
    "                                                  X_train, \n",
    "                                                  y_train, \n",
    "                                                  cv=cv, \n",
    "                                                  n_jobs = -1)\n",
    "    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n",
    "    return train_pred, test_pred, acc, acc_cv, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "import datetime\n",
    "import time \n",
    "start_time = time.time()\n",
    "train_pred_log, test_pred_log, acc_log, acc_cv_log, probs_log = fit_ml_algo(LogisticRegression(n_jobs = -1), \n",
    "                                                                     Xtrain, \n",
    "                                                                    ytrain, \n",
    "                                                                   Xvalid,  \n",
    "                                                                    yvalid, \n",
    "                                                                 10)\n",
    "log_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_log)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( Xtrain.shape , Xvalid.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(yvalid, probs_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors\n",
    "start_time = time.time()\n",
    "train_pred_knn, test_pred_knn, acc_knn, acc_cv_knn, probs_knn = fit_ml_algo(KNeighborsClassifier(n_neighbors = 3,\n",
    "                                                                                                 n_jobs = -1), \n",
    "                                                                                                 Xtrain, \n",
    "                                                                                                 ytrain, \n",
    "                                                                                                 Xvalid,\n",
    "                                                                                                yvalid,\n",
    "                                                                                                 10)\n",
    "knn_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_knn)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Trees\n",
    "start_time = time.time()\n",
    "train_pred_gbt, test_pred_gbt, acc_gbt, acc_cv_gbt, probs_gbt = fit_ml_algo(GradientBoostingClassifier(), \n",
    "                                                                 Xtrain, \n",
    "                                                                 ytrain, \n",
    "                                                                 Xvalid, \n",
    "                                                                yvalid,\n",
    "                                                                 10)\n",
    "gbt_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_gbt)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))\n",
    "print (metrics.classification_report(ytrain, train_pred_gbt))\n",
    "print (metrics.classification_report(yvalid, test_pred_gbt)) \n",
    "plot_roc_curve(yvalid, probs_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "start_time = time.time()\n",
    "rfc = RandomForestClassifier(n_estimators=10, \n",
    "                             min_samples_leaf=2,\n",
    "                             min_samples_split=17, \n",
    "                             criterion='gini', \n",
    "                             max_features=5)\n",
    "train_pred_rf, test_pred_rf, acc_rf, acc_cv_rf, probs_rf = fit_ml_algo(rfc, \n",
    "                                                             Xtrain, \n",
    "                                                             ytrain, \n",
    "                                                             Xvalid, \n",
    "                                                            yvalid ,\n",
    "                                                             10)\n",
    "rf_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_rf)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_rf)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=rf_time))\n",
    "print (metrics.classification_report(ytrain, train_pred_rf))\n",
    "print (metrics.classification_report(yvalid, test_pred_rf))\n",
    "plot_roc_curve(yvalid, probs_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "start_time = time.time()\n",
    "train_pred_svc, test_pred_svc, acc_rbf_svc, acc_cv_rbf_svc, _ = fit_ml_algo(SVC( kernel = \"rbf\") , \n",
    "                                                                                    Xtrain, \n",
    "                                                                                    ytrain, \n",
    "                                                                                    Xvalid, \n",
    "                                                                                    yvalid ,\n",
    "                                                                                   10)\n",
    "\n",
    "\n",
    "                                                                            \n",
    "print(\"Accuracy: %s\" % acc_rbf_svc)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_rbf_svc)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=time.time() - start_time))\n",
    "print (metrics.classification_report(ytrain, train_pred_rf))\n",
    "print (metrics.classification_report(yvalid, test_pred_rf))\n",
    "plot_roc_curve(yvalid, probs_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# read data into Xgboost DMatrix format\n",
    "dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "dtest = xgb.DMatrix(Xvalid, label=yvalid)\n",
    "\n",
    "# specify parameters via map\n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth':10,\n",
    "    'lambda':1,\n",
    "    'subsample':0.75,\n",
    "    'colsample_bytree':0.75,\n",
    "    'min_child_weight':2,\n",
    "    'eta': 0.005,\n",
    "    'seed':0,\n",
    "    'nthread':8,\n",
    "     'silent':1\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain,'train')]\n",
    "bst=xgb.train(params,dtrain,num_boost_round=5,evals=watchlist)\n",
    "\n",
    "\n",
    "#输出概率\n",
    "# 预测test \n",
    "\n",
    "ypred = bst.predict(dtest)\n",
    "y_hat  = []\n",
    "for i in ypred :\n",
    "    if i> 0.5 :\n",
    "        y_hat.append( 1) \n",
    "    else :\n",
    "        y_hat.append(0) \n",
    "accuracy = np.sum(y_hat == yvalid) / ypred.shape[0]\n",
    "print('Test accuracy = {}'.format(accuracy))\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "\n",
    "\n",
    "# 预测train \n",
    "\n",
    "ypred = bst.predict(dtrain)\n",
    "y_hat  = []\n",
    "for i in ypred :\n",
    "    if i> 0.5 :\n",
    "        y_hat.append( 1) \n",
    "    else :\n",
    "        y_hat.append(0) \n",
    "accuracy = np.sum(y_hat == ytrain) / ypred.shape[0]\n",
    "print('Train accuracy = {}'.format(accuracy))\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "import numpy as np \n",
    "\n",
    "params={\n",
    "    'max_depth': [ 5,10 ,15, 20,25 ,30 ,35 ]   , \n",
    "    'lambda': [0.1, 0.5 ,1, 2, 5, 10 ] , \n",
    "    'subsample': [0.6 , 0.75, 0.8,0.95 ]  , \n",
    "    'min_child_weight':[1,2,3] , \n",
    "    'eta': [1e-4, 3e-4, 1e-3  ,3e-3 , 1e-2, 3e-2 , 1e-1],\n",
    "}\n",
    "        \n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic' ,booster='gbtree')\n",
    "clf = GridSearchCV(xgb_model,params,verbose=1 ,n_jobs=1)\n",
    "clf.fit(Xtrain,ytrain)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'booster':'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth':10,\n",
    "    'lambda':1,\n",
    "    'subsample':0.75,\n",
    "    'colsample_bytree':0.75,\n",
    "    'min_child_weight':2,\n",
    "    'eta': 0.01,\n",
    "    'seed':0,\n",
    "    'nthread':8,\n",
    "     'silent':1\n",
    "}\n",
    "\n",
    "bst = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "watchlist = [(dtrain,'train')]\n",
    "bst=xgb.train(params,dtrain,num_boost_round=10 ,evals=watchlist)\n",
    "#输出概率\n",
    "# 预测test \n",
    "\n",
    "ypred = bst.predict(dtest)\n",
    "y_hat  = []\n",
    "for i in ypred :\n",
    "    if i> 0.5 :\n",
    "        y_hat.append( 1) \n",
    "    else :\n",
    "        y_hat.append(0) \n",
    "accuracy = np.sum(y_hat == yvalid) / ypred.shape[0]\n",
    "print('Test accuracy = {}'.format(accuracy))\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))\n",
    "# 预测train \n",
    "ypred = bst.predict(dtrain)\n",
    "y_hat  = []\n",
    "for i in ypred :\n",
    "    if i> 0.5 :\n",
    "        y_hat.append( 1) \n",
    "    else :\n",
    "        y_hat.append(0) \n",
    "accuracy = np.sum(y_hat == ytrain) / ypred.shape[0]\n",
    "print('Train accuracy = {}'.format(accuracy))\n",
    "end_time = time.time()\n",
    "print('The training time = {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_data = data.loc[ data['is_test'] , :]\n",
    "Xtest_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_data.pop('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = [ 'Pclass', 'Sex', 'Age', 'SibSp', 'Ticket' , 'Parch', 'Fare', 'Embarked','family_size', 'average_Fare']\n",
    "Xlast =  Xtest_data.loc[: ,train_label  ]\n",
    "for tag  in ['Sex' ,'Embarked' ,'Ticket' , 'family_size'] :\n",
    "    Xlast[tag] = pd.Categorical(Xlast[tag] ).codes \n",
    "Xlast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlast = xgb.DMatrix(Xlast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltrain  = pd.concat([Xtrain ,Xvalid] , axis = 0 )\n",
    "dlast_train = xgb.DMatrix(alltrain  )\n",
    "ypred = bst.predict(dlast_train )\n",
    "y_hat_new  = []\n",
    "for i in ypred :\n",
    "    if i> 0.5 :\n",
    "        y_hat_new.append( 1) \n",
    "    else :\n",
    "        y_hat_new.append(0) \n",
    "\n",
    "print (\" last predict \\n\")\n",
    "\n",
    "res = dict()\n",
    "for i in range(len( y_hat_new)) : \n",
    "    print ( i+1,  y_hat_new[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_test_id  = data.loc[ data['is_test'] ,'PassengerId' ]\n",
    "y_hat_new  = []\n",
    "for i in ypred :\n",
    "    if i> 0.5 :\n",
    "        y_hat_new.append( 1) \n",
    "    else :\n",
    "        y_hat_new.append(0) \n",
    "\n",
    "res = dict()\n",
    "print (\" last predict \\n\")\n",
    "for i in range(len( y_hat_new)) : \n",
    "    print (  last_test_id[i] ,  y_hat_new[i])\n",
    "    res[last_test_id[i] ] =  y_hat_new[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(res.items()),columns = ['PassengerId','Survived']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('gender_submission.csv' ,index =0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
